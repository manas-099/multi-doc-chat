{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ccc93211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.5 environment at: C:\\Users\\manas\\OneDrive\\Desktop\\multi-doc-chat\\.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m6 packages\u001b[0m \u001b[2min 2.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! uv pip install langchain langchain-google-genai tiktoken rapidocr-onnxruntime python-dotenv langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e75b1cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! uv pip install -q sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aeac7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "! uv pip install -qU langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a47e4524",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"] =os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66e61e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"rag_data.txt\", encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5339b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94047918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,       # max characters per chunk\n",
    "    chunk_overlap=20,     # how much text to overlap between chunks\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],  # default fallback split hierarchy\n",
    ")\n",
    "\n",
    "\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "06626c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'rag_data.txt'}, page_content='Recent AI developments — summary (compiled Nov 6, 2025)'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='Overview:'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='- This file summarizes recent developments (2024–2025) in large language models (LLMs), embedding models, and agentic AI (agent frameworks, products, and industry trends). Each major claim includes a'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='claim includes a short citation tag for reference to original reporting.'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='1) New / notable LLM releases and trends'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content=\"- LLaMA / Llama 4 family: Meta's Llama 4 (and follow-ups like Llama 4 Scout / Maverick) expanded multimodal capabilities and used mixture-of-experts architectures to scale efficiency and add\"),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='efficiency and add image/video understanding. \\ue200cite\\ue202turn0search0\\ue202turn0search9\\ue201'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content=\"- Google's Gemini family (Gemini 2.5 / Gemma variants) and related Gemma models remained major contenders in 2025 with multimodal and on-device focuses. \\ue200cite\\ue202turn0search6\\ue202turn0search9\\ue201\"),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='- OpenAI and other firms continued iterative releases (GPT series and O-series variants), with community leaderboards showing GPT-5/GPT-4 derivatives and vendor competitors like Grok 4 and Claude 4'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='Grok 4 and Claude 4 across reasoning/coding benchmarks. \\ue200cite\\ue202turn0search6\\ue202turn0search12\\ue201'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='- Alibaba (Qwen3), Anthropic (Claude 4: Sonnet & Opus), Mistral, Falcon, and other open-source and proprietary models continued to expand available choices with varied licensing and sizes.'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='and sizes. \\ue200cite\\ue202turn0search9\\ue202turn0search2\\ue201'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='2) Embedding models (2024–2025 highlights)'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='- OpenAI introduced the text-embedding-3 family (3-small and 3-large) and continued embedding API updates aimed at lower cost and better performance for retrieval/RAG use cases. \\ue200cite\\ue202turn0search5\\ue201'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content=\"- Specialized and on-device embedding releases (e.g., Google's EmbeddingGemma announced Sept 2025) targeted small-footprint, multilingual embeddings for on-device semantic search and RAG scenarios.\"),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='and RAG scenarios. \\ue200cite\\ue202turn0search13\\ue201'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='- The embedding ecosystem matured: cloud APIs (OpenAI, Cohere, Google), new competitors (Voyage AI, Ollama, Jina), and open-weight models offered tradeoffs between latency, cost, and privacy.'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='cost, and privacy. Benchmarks and guides in 2025 emphasized Recall@k metrics, p95 latency, and quantization/optimization for production. \\ue200cite\\ue202turn0search1\\ue202turn0search7\\ue202turn0search3\\ue201'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='3) Agentic AI — frameworks, products, and industry trends'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='- Agentic AI (systems that autonomously plan and execute multi-step tasks) was a prominent strategic trend in 2025. Vendors rolled out SDKs, production-focused toolkits, and enterprise agents (e.g.,'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content=\"agents (e.g., Anthropic 'Skills', OpenAI AgentKit announcements reported alongside other vendor efforts). \\ue200cite\\ue202turn0news37\\ue202turn0news37\\ue201\"),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='- Major platform and vertical integrations: companies like Dalet (media workflows), Salesforce (Agentforce / agent frameworks), IBM, Oracle, and others published guides and product offerings to build'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='offerings to build or host agentic capabilities for specific enterprise workflows. \\ue200cite\\ue202turn0news39\\ue202turn0search8\\ue202turn0search11\\ue201'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content=\"- Caution & realism: industry analysts warned of 'agent washing' and predicted many agentic projects would be scrapped or fail to deliver ROI without clear business value and engineering investment.\"),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='investment. Gartner estimated over 40% of agentic AI projects might be scrapped by end of 2027 due to cost and unclear value. \\ue200cite\\ue202turn0news29\\ue201'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='4) Practical takeaways for engineers, product managers, and researchers'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='- Choose the right model family by task: reasoning/math/coding tasks still favor top-tier closed models (GPT family, Grok) on many benchmarks; Llama and other open models offer customization and'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='customization and lower-cost fine-tuning. \\ue200cite\\ue202turn0search6\\ue202turn0search12\\ue201'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='- For retrieval and RAG, benchmark embeddings on your data (Recall@k, latency) and consider hybrid pipelines (cheap+high-quality embeddings, coarse-to-fine retrieval). Quantize embeddings and use'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='embeddings and use two-stage retrieval to control cost/latency. \\ue200cite\\ue202turn0search3\\ue202turn0search1\\ue201'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='- When building agentic systems: start with clear KPIs, limit scope (one vertical workflow), invest in orchestration, tool safety & monitoring, and avoid premature generalization claims (watch for'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content=\"claims (watch for 'agent washing'). Consider SDKs from major vendors but plan for heavy engineering and maintenance. \\ue200cite\\ue202turn0news29\\ue202turn0search11\\ue201\"),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='5) Further reading and recent sources (selected)\\n- Wikipedia: List of large language models (updated through 2025). \\ue200cite\\ue202turn0search9\\ue201'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='- OpenAI: New embedding models and API updates (text-embedding-3 announcement). \\ue200cite\\ue202turn0search5\\ue201'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='- Reuters: Gartner analysis on agentic AI project failures and market caution (Jun 25, 2025). \\ue200cite\\ue202turn0news29\\ue201'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content=\"- The Verge: Anthropic 'Skills' feature and Claude agent developments (recent coverage). \\ue200cite\\ue202turn0news37\\ue201\"),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='- Artsmart / Elephas / Medium guides: comparative overviews of embedding model performance and recommendations (2025 summaries). \\ue200cite\\ue202turn0search1\\ue202turn0search7\\ue202turn0search3\\ue201'),\n",
       " Document(metadata={'source': 'rag_data.txt'}, page_content='(End of file)')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d837d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6d2a75f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.5 environment at: C:\\Users\\manas\\OneDrive\\Desktop\\multi-doc-chat\\.venv\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m3 packages\u001b[0m \u001b[2min 767ms\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m faiss-cpu \u001b[2m(17.3MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m faiss-cpu\n",
      "\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 13.30s\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 91ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfaiss-cpu\u001b[0m\u001b[2m==1.12.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! uv pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b8ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d8751211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embedding_dim = len(embeddings.embed_query(\"hello world\"))\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# vector_store = FAISS(\n",
    "#     embedding_function=embeddings,\n",
    "#     index=index,\n",
    "#     docstore=InMemoryDocstore(),\n",
    "#     index_to_docstore_id={},\n",
    "# )\n",
    "vectorstore=FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "55909f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "191087a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='fb762ded-dbfd-4fd1-9c36-caac4a6f6a32', metadata={'source': 'rag_data.txt'}, page_content='2) Embedding models (2024–2025 highlights)'),\n",
       "  np.float32(0.79484797)),\n",
       " (Document(id='b0444df2-2a95-41d4-bb7a-3f3686b69e1c', metadata={'source': 'rag_data.txt'}, page_content=\"- Specialized and on-device embedding releases (e.g., Google's EmbeddingGemma announced Sept 2025) targeted small-footprint, multilingual embeddings for on-device semantic search and RAG scenarios.\"),\n",
       "  np.float32(0.77727056)),\n",
       " (Document(id='6c8b2403-1488-47b5-88ff-fd6bd1883663', metadata={'source': 'rag_data.txt'}, page_content='- OpenAI: New embedding models and API updates (text-embedding-3 announcement). \\ue200cite\\ue202turn0search5\\ue201'),\n",
       "  np.float32(0.76711977)),\n",
       " (Document(id='48e28468-be6e-4de3-85b5-48a921626e1e', metadata={'source': 'rag_data.txt'}, page_content='- This file summarizes recent developments (2024–2025) in large language models (LLMs), embedding models, and agentic AI (agent frameworks, products, and industry trends). Each major claim includes a'),\n",
       "  np.float32(0.76282215)),\n",
       " (Document(id='ab3fa1ec-cd74-4f25-9f99-0a3b171affdb', metadata={'source': 'rag_data.txt'}, page_content='embeddings and use two-stage retrieval to control cost/latency. \\ue200cite\\ue202turn0search3\\ue202turn0search1\\ue201'),\n",
       "  np.float32(0.7626987))]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=vectorstore.similarity_search_with_relevance_scores(query=\"what is the recent embedding model\",k=5)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "81556bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "\n",
    "template=\"\"\"you are an ai assistant .your job is to answer the query of user. you are given context .based on the context ans the queries. use 50 words to answer the query.\n",
    "here is the context:{context}\n",
    "here is the query:{query}\n",
    "\"\"\"\n",
    "prompt=ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain=(\n",
    "    {\"context\":retriever,\"query\":RunnablePassthrough()}\n",
    "    |prompt\n",
    "    |llm\n",
    "    |StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9784819a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Recent embedding model developments (2024–2025) include OpenAI's new embedding models and API updates, such as `text-embedding-3`. Google also announced `EmbeddingGemma` (Sept 2025), which focuses on specialized, small-footprint, multilingual embeddings for on-device semantic search and RAG scenarios. These advancements aim to enhance performance and utility in various AI applications.\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chain.invoke(\"tell me about recent embedding model developed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4cb66e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mulitdocchatapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
